{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import os\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from pymongo import UpdateOne\n",
    "from pymongo import MongoClient\n",
    "import sys\n",
    "import time\n",
    "from sner import Ner\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "\n",
    "LOCATIONS = [\"eu\", \"oxford\", \"britain\"]\n",
    "locations_set = set(LOCATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    client = MongoClient(\"mongodb://igenie_readwrite:igenie@35.197.204.103:27017/dax_gcp\")\n",
    "    db = client[\"dax_gcp\"]\n",
    "    collection = db[\"tweets\"]\n",
    "\n",
    "    sia = SIA()\n",
    "    #st = StanfordNERTagger(argv[0],argv[1],encoding='utf-8')\n",
    "    #tagger = Ner(host='localhost', port=9199)\n",
    "    #tokenizer = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=False)\n",
    "    \n",
    "    #get classification stuff\n",
    "    '''\n",
    "    model_dir = str(os.path.join(\"..\",\"..\",\"models\"))\n",
    "    data_dir = str(os.path.join(\"..\", \"..\", \"data_dir\", \"twitter_naive_bayes\"))\n",
    "    text_clf = joblib.load(str(os.path.join(model_dir, \"naive_bayes.plk\")))\n",
    "    count_vectorizer = joblib.load(str(os.path.join(model_dir, \"count_vectorizer.plk\")))\n",
    "    tf_transformer = joblib.load(str(os.path.join(model_dir, \"tf_transformer.plk\")))\n",
    "\n",
    "    \n",
    "\n",
    "    cursor = collection.find({\"constituent\":\"BMW\", \"relevance\":1},{\"_id\":1,\"text\":1,\n",
    "                                                    \"text_en\":1,\"semi_processed_text\":1,\n",
    "                                                   \"processed_text\":1})\n",
    "    \n",
    "    \n",
    "    cursor = collection.find({\"constituent\":\"BMW\",\n",
    "                             \"nltk_sentiment_numeric\":{\"$exists\":False}},\n",
    "                             {\"_id\":1,\"text\":1,\"text_en\":1,\"semi_processed_text\":1,\n",
    "                              \"processed_text\":1})\n",
    "    '''\n",
    "    cursor = collection.find({\"constituent\":\"EON\",\n",
    "                            \"date\":{\"$gte\":datetime(2017,9,12),\n",
    "                                    \"$lte\":datetime(2017,9,14)}},\n",
    "                            {\"_id\":1,\"text\":1,\"date\":1,\n",
    "                             \"nltk_sentiment_numeric\":1,\n",
    "                             \"nltk_sentiment_score\":1,\n",
    "                            \"text_en\":1})\n",
    "    \n",
    "    operations = [] \n",
    "    all_tweets = list(cursor)\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    text_list = []\n",
    "    for t in all_tweets:\n",
    "        text_list.append(\" \".join(t[\"processed_text\"]))\n",
    "        \n",
    "    relevance = get_relevance(text_clf,count_vectorizer,tf_transformer,text_list)\n",
    "    '''\n",
    "    \n",
    "\n",
    "    records = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0,len(all_tweets)):\n",
    "        doc = all_tweets[i]\n",
    "        \n",
    "        \n",
    "        if \"text_en\" in doc:\n",
    "            sentiment_score = get_nltk_sentiment(doc[\"text_en\"],sia)\n",
    "        else:\n",
    "            sentiment_score = get_nltk_sentiment(doc[\"text\"],sia)\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        if \"text_en\" in doc:\n",
    "            tags_temp = get_tags(doc[\"text_en\"], tagger)\n",
    "        else:\n",
    "            tags_temp = get_tags(doc[\"text\"],tagger)\n",
    "            \n",
    "        tags = process_tags(tags_temp)\n",
    "        '''\n",
    "\n",
    "        new_values = {}\n",
    "        new_values[\"nltk_sentiment_numeric\"] = sentiment_score\n",
    "        \n",
    "        if sentiment_score < -0.25:\n",
    "            new_values[\"nltk_sentiment_score\"] = \"Negative\"\n",
    "        elif sentiment_score > 0.25:\n",
    "            new_values[\"nltk_sentiment_score\"] = \"Positive\"\n",
    "        else:\n",
    "            new_values[\"nltk_sentiment_score\"] = \"Neutral\"\n",
    "            \n",
    "        if \"Andrew\" in doc[\"text\"]:\n",
    "            print(new_values)\n",
    "        \n",
    "        #new_values[\"relevance\"] = int(relevance[i])\n",
    "        #new_values[\"tag_LOCATION\"] = list()\n",
    "        #new_values[\"tag_PERSON\"] = list()\n",
    "        #new_values[\"tag_ORGANIZATION\"] = list()\n",
    "        #new_values[\"tag_MONEY\"] = list()\n",
    "        #new_values[\"tag_PERCENT\"] = list()\n",
    "        #new_values[\"tag_DATE\"] = list()\n",
    "        #new_values[\"tag_TIME\"] = list()\n",
    "\n",
    "        ''''\n",
    "        for word, tag in tags:\n",
    "            if tag != \"O\":\n",
    "                new_values[\"tag_\" + tag].append(word)\n",
    "        '''\n",
    "\n",
    "        operations.append(\n",
    "            UpdateOne({\"_id\": doc[\"_id\"]}, {\"$set\": new_values})\n",
    "        )\n",
    "\n",
    "        # Send once every 1000 in batch\n",
    "        if (len(operations) == 1000):\n",
    "            print(\"Performing bulk write\")\n",
    "            collection.bulk_write(operations, ordered=False)\n",
    "            operations = []\n",
    "            records += 1000\n",
    "            print(\"Write done. Saved {} records\".format(records))\n",
    "\n",
    "    if (len(operations) > 0):\n",
    "        collection.bulk_write(operations, ordered=False)\n",
    "        #pass\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Processed {} records\".format(records))\n",
    "\n",
    "\n",
    "def get_nltk_sentiment(semi_processed_text,sia):\n",
    "    res = sia.polarity_scores(semi_processed_text)\n",
    "\n",
    "    return res[\"compound\"]\n",
    "\n",
    "def get_tags(text, tagger):\n",
    "    new_text = text.replace('â‚¬','$')\n",
    "    new_text = new_text.replace('#', ' ')\n",
    "    #tokenized_text = tokenizer.tokenize(new_text)\n",
    "    #classified_text = st.tag(tokenized_text)\n",
    "    classified_text = tagger.get_entities(new_text)\n",
    "    return classified_text\n",
    "\n",
    "def get_relevance(text_clf, count_vectorizer, tf_transformer, processed_text):\n",
    "    X_test_counts = count_vectorizer.transform(processed_text)\n",
    "    X_test_tf = tf_transformer.transform(X_test_counts)\n",
    "    predicted = text_clf.predict(X_test_tf)\n",
    "    return predicted\n",
    "\n",
    "def process_tags(tags):\n",
    "    all_tags = []\n",
    "    temp = []\n",
    "\n",
    "    for word, tag in tags:\n",
    "        if word.lower() in locations_set:\n",
    "            tag = \"LOCATION\"\n",
    "        if \"http\" in word:\n",
    "            continue\n",
    "        if tag == \"O\":\n",
    "            if temp:\n",
    "                all_tags.append((\" \".join(temp[1]), temp[0]))\n",
    "                temp = []\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if temp:\n",
    "                if temp[0] == tag:\n",
    "                    temp[1].append(word.lower())\n",
    "                else:\n",
    "                    all_tags.append((\" \".join(temp[1]), temp[0]))\n",
    "                    temp = [None]*2\n",
    "                    temp[0] = tag\n",
    "                    temp[1] = [word.lower()]\n",
    "            else:\n",
    "                temp = [None]*2\n",
    "                temp[0] = tag\n",
    "                temp[1] = [word.lower()]\n",
    "\n",
    "    #pprint(all_tags)\n",
    "    return all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "{'nltk_sentiment_score': 'Neutral', 'nltk_sentiment_numeric': 0.0}\n",
      "--- 2.261082410812378 seconds ---\n",
      "Processed 0 records\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}