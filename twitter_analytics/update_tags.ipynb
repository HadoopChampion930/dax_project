{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import os\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from pymongo import UpdateOne\n",
    "from pymongo import MongoClient\n",
    "import sys\n",
    "import time\n",
    "from sner import Ner\n",
    "from pprint import pprint\n",
    "\n",
    "LOCATIONS = [\"eu\", \"oxford\", \"britain\"]\n",
    "locations_set = set(LOCATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    client = MongoClient(\"mongodb://igenie_readwrite:igenie@35.189.101.142:27017/dax_gcp\")\n",
    "    db = client[\"dax_gcp\"]\n",
    "    collection = db[\"tweets\"]\n",
    "\n",
    "    sia = SIA()\n",
    "    #st = StanfordNERTagger(argv[0],argv[1],encoding='utf-8')\n",
    "    tagger = Ner(host='localhost', port=9199)\n",
    "    #tokenizer = TweetTokenizer(preserve_case=True, reduce_len=True, strip_handles=False)\n",
    "    \n",
    "    #get classification stuff\n",
    "    model_dir = str(os.path.join(\"..\",\"..\",\"models\"))\n",
    "    data_dir = str(os.path.join(\"..\", \"..\", \"data_dir\", \"twitter_naive_bayes\"))\n",
    "    text_clf = joblib.load(str(os.path.join(model_dir, \"naive_bayes.plk\")))\n",
    "    count_vectorizer = joblib.load(str(os.path.join(model_dir, \"count_vectorizer.plk\")))\n",
    "    tf_transformer = joblib.load(str(os.path.join(model_dir, \"tf_transformer.plk\")))\n",
    "\n",
    "    operations = []\n",
    "    cursor = collection.find({\"constituent\":\"BMW\", \"relevance\":1},{\"_id\":1,\"text\":1,\n",
    "                                                    \"text_en\":1,\"semi_processed_text\":1,\n",
    "                                                   \"processed_text\":1})\n",
    "    all_tweets = list(cursor)\n",
    "    \n",
    "    text_list = []\n",
    "    for t in all_tweets:\n",
    "        text_list.append(\" \".join(t[\"processed_text\"]))\n",
    "        \n",
    "    relevance = get_relevance(text_clf,count_vectorizer,tf_transformer,text_list)\n",
    "\n",
    "    records = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0,len(all_tweets)):\n",
    "        doc = all_tweets[i]\n",
    "        sentiment_score = get_nltk_sentiment(doc[\"semi_processed_text\"],sia)\n",
    "        \n",
    "        \n",
    "        if \"text_en\" in doc:\n",
    "            tags_temp = get_tags(doc[\"text_en\"], tagger)\n",
    "        else:\n",
    "            tags_temp = get_tags(doc[\"text\"],tagger)\n",
    "            \n",
    "        tags = process_tags(tags_temp)\n",
    "\n",
    "        new_values = {}\n",
    "        new_values[\"nltk_sentiment_numeric\"] = sentiment_score\n",
    "        new_values[\"relevance\"] = int(relevance[i])\n",
    "        new_values[\"tag_LOCATION\"] = list()\n",
    "        new_values[\"tag_PERSON\"] = list()\n",
    "        new_values[\"tag_ORGANIZATION\"] = list()\n",
    "        new_values[\"tag_MONEY\"] = list()\n",
    "        new_values[\"tag_PERCENT\"] = list()\n",
    "        new_values[\"tag_DATE\"] = list()\n",
    "        new_values[\"tag_TIME\"] = list()\n",
    "\n",
    "        for word, tag in tags:\n",
    "            if tag != \"O\":\n",
    "                new_values[\"tag_\" + tag].append(word)\n",
    "\n",
    "        operations.append(\n",
    "            UpdateOne({\"_id\": doc[\"_id\"]}, {\"$set\": new_values})\n",
    "        )\n",
    "\n",
    "        # Send once every 1000 in batch\n",
    "        if (len(operations) == 1000):\n",
    "            print(\"Performing bulk write\")\n",
    "            collection.bulk_write(operations, ordered=False)\n",
    "            operations = []\n",
    "            records += 1000\n",
    "            print(\"Write done. Saved {} records\".format(records))\n",
    "\n",
    "    if (len(operations) > 0):\n",
    "        collection.bulk_write(operations, ordered=False)\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    print(\"Processed {} records\".format(records))\n",
    "\n",
    "\n",
    "def get_nltk_sentiment(semi_processed_text,sia):\n",
    "    res = sia.polarity_scores(semi_processed_text)\n",
    "\n",
    "    return res[\"compound\"]\n",
    "\n",
    "def get_tags(text, tagger):\n",
    "    new_text = text.replace('â‚¬','$')\n",
    "    new_text = new_text.replace('#', ' ')\n",
    "    #tokenized_text = tokenizer.tokenize(new_text)\n",
    "    #classified_text = st.tag(tokenized_text)\n",
    "    classified_text = tagger.get_entities(new_text)\n",
    "    return classified_text\n",
    "\n",
    "def get_relevance(text_clf, count_vectorizer, tf_transformer, processed_text):\n",
    "    X_test_counts = count_vectorizer.transform(processed_text)\n",
    "    X_test_tf = tf_transformer.transform(X_test_counts)\n",
    "    predicted = text_clf.predict(X_test_tf)\n",
    "    return predicted\n",
    "\n",
    "def process_tags(tags):\n",
    "    all_tags = []\n",
    "    temp = []\n",
    "\n",
    "    for word, tag in tags:\n",
    "        if word.lower() in locations_set:\n",
    "            tag = \"LOCATION\"\n",
    "        if \"http\" in word:\n",
    "            continue\n",
    "        if tag == \"O\":\n",
    "            if temp:\n",
    "                all_tags.append((\" \".join(temp[1]), temp[0]))\n",
    "                temp = []\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            if temp:\n",
    "                if temp[0] == tag:\n",
    "                    temp[1].append(word.lower())\n",
    "                else:\n",
    "                    all_tags.append((\" \".join(temp[1]), temp[0]))\n",
    "                    temp = [None]*2\n",
    "                    temp[0] = tag\n",
    "                    temp[1] = [word.lower()]\n",
    "            else:\n",
    "                temp = [None]*2\n",
    "                temp[0] = tag\n",
    "                temp[1] = [word.lower()]\n",
    "\n",
    "    #pprint(all_tags)\n",
    "    return all_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing bulk write\n",
      "Write done. Saved 1000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 2000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 3000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 4000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 5000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 6000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 7000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 8000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 9000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 10000 records\n",
      "Performing bulk write\n",
      "Write done. Saved 11000 records\n",
      "--- 100.89949059486389 seconds ---\n",
      "Processed 11000 records\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
